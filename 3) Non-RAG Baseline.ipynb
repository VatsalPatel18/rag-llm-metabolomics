{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (if required)\n",
    "!pip install llama-index-vector-stores-faiss\n",
    "!pip install llama_index llama-index-llms-openai\n",
    "!pip install PyPDF2 faiss-cpu -U langchain-community rouge-score python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Step 1: Configuration\n",
    "###############################################\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API Key\n",
    "openai_api_key = # Place yuur API key here \n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# OpenAI model configuration\n",
    "LLM_MODEL = \"gpt-4\"  # Default model\n",
    "TEMPERATURE = 0  # Control randomness\n",
    "\n",
    "###############################################\n",
    "# Step 2: Query Function\n",
    "###############################################\n",
    "def test_llama_index_openai_llm(prompt, model=LLM_MODEL, temperature=TEMPERATURE):\n",
    "    \"\"\"\n",
    "    Query OpenAI LLM for direct question answering with concise responses.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Input prompt to query.\n",
    "        model (str): OpenAI model to use.\n",
    "        temperature (float): Temperature parameter for response generation.\n",
    "\n",
    "    Returns:\n",
    "        str: Model's response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Modify the prompt to ensure concise, one-word responses\n",
    "        concise_prompt = f\"{prompt}\\nAnswer in one word or less:\"\n",
    "\n",
    "        llm = OpenAI(model=model, temperature=temperature)\n",
    "        response = llm.complete(concise_prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    \"Which biomarker is significantly elevated in the plasma of Gaucher Disease Type 1 patients?\",\n",
    "    \"What therapy reduces urinary GlcSph levels in Gaucher Disease patients?\",\n",
    "    \"Which biomarker is considered the gold standard for monitoring Gaucher Disease Type 1?\",\n",
    "    \"What technique is used to quantify lyso-Gb1 and its analogs in plasma?\",\n",
    "    \"Which urinary biomarker is highlighted for monitoring Gaucher Disease progression?\",\n",
    "    \"What urinary biomarker class is elevated in untreated Gaucher Disease patients?\",\n",
    "    \"What type of molecule is lyso-Gb1, which is associated with Gaucher Disease Type 1?\",\n",
    "    \"Which class of biomarkers does lyso-Gb1 belong to in the context of Gaucher Disease Type 1?\",\n",
    "    \"Which amino acids are elevated in patients with NASH compared to NAFLD?\",\n",
    "    \"What metabolic pathway is closely associated with NASH progression?\",\n",
    "    \"What urinary biomarker distinguishes between NAFLD and NASH?\",\n",
    "    \"What metabolic pathway is altered in the progression from NAFLD to NASH?\",\n",
    "    \"Which sulfated steroid increases with the progression of fibrosis in NAFLD?\",\n",
    "    \"Which metabolite ratio is associated with fibrosis severity in NAFLD?\",\n",
    "    \"Which biomarker is commonly used for early detection of Type 2 Diabetes (T2D)?\",\n",
    "    \"What metabolic pathway is associated with 3-hydroxybutyrate in T2D?\",\n",
    "    \"Which amino acids are identified as predictors of future diabetes in metabolomic studies?\",\n",
    "    \"What diagnostic method is used for metabolic profiling in diabetes research?\",\n",
    "    \"Which metabolite is associated with the progression of diabetic kidney disease?\",\n",
    "    \"What technology is used for identifying lipid metabolism-related biomarkers in diabetes?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Glucosylsphingosine (GlcSph).\",\n",
    "    \"Enzyme Replacement Therapy (ERT).\",\n",
    "    \"Lyso-Gb1.\",\n",
    "    \"UPLC-MS/MS.\",\n",
    "    \"Lyso-Gb1 analogs.\",\n",
    "    \"Polycyclic Lyso-Gb1 analogs.\",\n",
    "    \"A glucosylsphingosine derivative.\",\n",
    "    \"Lipid biomarkers.\",\n",
    "    \"Glutamate and phenylalanine.\",\n",
    "    \"Amino acid metabolism.\",\n",
    "    \"Pyroglutamic acid.\",\n",
    "    \"Pentose phosphate pathway.\",\n",
    "    \"16-OH-DHEA-S.\",\n",
    "    \"16-OH-DHEA-S/DHEA-S.\",\n",
    "    \"HbA1c.\",\n",
    "    \"Ketogenesis.\",\n",
    "    \"Branched-chain amino acids (BCAAs) like isoleucine, leucine, and valine.\",\n",
    "    \"Nuclear Magnetic Resonance (NMR) spectroscopy.\",\n",
    "    \"Phenylalanine.\",\n",
    "    \"Liquid Chromatography-Mass Spectrometry (LC-MS).\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Step 4: Evaluation Function\n",
    "###############################################\n",
    "def evaluate_responses(queries, ground_truths):\n",
    "    \"\"\"\n",
    "    Evaluate OpenAI LLM responses against ground truths using ROUGE and BLEU.\n",
    "\n",
    "    Args:\n",
    "        queries (list): List of query strings.\n",
    "        ground_truths (list): Corresponding ground truth answers.\n",
    "\n",
    "    Returns:\n",
    "        list: Evaluation metrics for each query-response pair.\n",
    "    \"\"\"\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for query, ground_truth in zip(queries, ground_truths):\n",
    "        response = test_llama_index_openai_llm(query)\n",
    "\n",
    "        # Extract the response text\n",
    "        if hasattr(response, \"text\"):\n",
    "            response_text = response.text.strip()\n",
    "        else:\n",
    "            response_text = str(response).strip()\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = rouge_scorer_instance.score(ground_truth, response_text)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = sentence_bleu(\n",
    "            [ground_truth.split()],\n",
    "            response_text.split(),\n",
    "            smoothing_function=smoothing_function\n",
    "        )\n",
    "\n",
    "        # Append results\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"response\": response_text,\n",
    "            \"rouge1\": rouge_scores['rouge1'].fmeasure,\n",
    "            \"rouge2\": rouge_scores['rouge2'].fmeasure,\n",
    "            \"rougeL\": rouge_scores['rougeL'].fmeasure,\n",
    "            \"bleu\": bleu_score\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "###############################################\n",
    "# Step 5: Save Results to CSV\n",
    "###############################################\n",
    "def save_results_to_csv(results, output_file=\"evaluation_metrics.csv\"):\n",
    "    \"\"\"\n",
    "    Save evaluation metrics to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        results (list): List of evaluation results.\n",
    "        output_file (str): Filepath for the output CSV.\n",
    "    \"\"\"\n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Query\", \"Ground Truth\", \"Response\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\"])\n",
    "\n",
    "        for result in results:\n",
    "            writer.writerow([\n",
    "                result[\"query\"],\n",
    "                result[\"ground_truth\"],\n",
    "                result[\"response\"],\n",
    "                f\"{result['rouge1']:.4f}\",\n",
    "                f\"{result['rouge2']:.4f}\",\n",
    "                f\"{result['rougeL']:.4f}\",\n",
    "                f\"{result['bleu']:.4f}\"\n",
    "            ])\n",
    "\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "###############################################\n",
    "# Step 6: Run Evaluation\n",
    "###############################################\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting evaluation of LLM responses...\")\n",
    "    evaluation_results = evaluate_responses(queries, ground_truths)\n",
    "\n",
    "    # Save evaluation metrics to a CSV file\n",
    "    save_results_to_csv(evaluation_results, \"query_based_evaluation_metrics.csv\")\n",
    "\n",
    "    print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ragas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness, context_recall, context_precision, answer_relevancy\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Convert `evaluation_results` to RAGAS-compatible format\n",
    "data_samples = {\n",
    "    'question': [result[\"query\"] for result in evaluation_results],\n",
    "    'answer': [result[\"response\"] for result in evaluation_results],  # Generated LLM responses\n",
    "    'contexts': [[] for _ in evaluation_results],  # Empty contexts as no retrieval is performed\n",
    "    'ground_truth': [result[\"ground_truth\"] for result in evaluation_results]  # Ground truth answers\n",
    "}\n",
    "\n",
    "# Step 2: Convert to `datasets.Dataset` object\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# Step 3: Evaluate using RAGAS metrics\n",
    "score = evaluate(dataset, metrics=[\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_relevancy\n",
    "])\n",
    "\n",
    "# Step 4: Convert the scores to a Pandas DataFrame and save as CSV\n",
    "df = score.to_pandas()\n",
    "output_score_csv_path = \"ragas_evaluation_metrics_no_context.csv\"\n",
    "df.to_csv(output_score_csv_path, index=False)\n",
    "\n",
    "print(f\"RAGAS evaluation scores saved to {output_score_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
