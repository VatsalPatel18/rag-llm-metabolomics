{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install faiss-cpu\n",
    "!pip install -U langchain-community\n",
    "!pip install rouge-score\n",
    "!pip install llama-index-vector-stores-faiss\n",
    "!pip install llama_index llama-index-llms-openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# If needed, install:\n",
    "# pip install openai\n",
    "openai_api_key =  # Replace with your actual key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: from \"/content/drive/MyDrive/sample_pdf_rag\" copy the folder and contents to current directory\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "source_path = \"/content/drive/MyDrive/sample_pdf_rag\"\n",
    "destination_path = os.getcwd()\n",
    "\n",
    "# Check if the source folder exists\n",
    "if os.path.exists(source_path):\n",
    "  try:\n",
    "    shutil.copytree(source_path, os.path.join(destination_path, \"sample_pdf_rag\"))\n",
    "    print(f\"Folder '{source_path}' copied to '{destination_path}' successfully.\")\n",
    "  except FileExistsError:\n",
    "      print(f\"Folder already exists in the destination directory\")\n",
    "  except OSError as e:\n",
    "    print(f\"Error copying folder: {e}\")\n",
    "else:\n",
    "  print(f\"Source folder '{source_path}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "###############################################\n",
    "# Step 1: Configuration\n",
    "###############################################\n",
    "PDF_FOLDER = \"/content/sample_pdf_rag\"  # folder containing your PDF files\n",
    "INDEX_SAVE_PATH = \"faiss_index.bin\"\n",
    "DOC_META_PATH = \"doc_metadata.json\"\n",
    "ARTICLE_ENCODER_MODEL = \"ncbi/MedCPT-Article-Encoder\"\n",
    "QUERY_ENCODER_MODEL = \"ncbi/MedCPT-Query-Encoder\"\n",
    "CROSS_ENCODER_MODEL = \"ncbi/MedCPT-Cross-Encoder\"\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "MAX_ARTICLE_LENGTH = 512\n",
    "MAX_QUERY_LENGTH = 64\n",
    "CHUNK_SIZE = 500  # number of tokens or words per chunk (adjust as needed)\n",
    "OVERLAP = 50      # number of tokens overlap between chunks (to maintain context)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "###############################################\n",
    "# Step 2: PDF Text Extraction Utility\n",
    "###############################################\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Extract all text from a PDF file\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "###############################################\n",
    "# Step 3: Text Chunking\n",
    "###############################################\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):\n",
    "    # This creates overlapping chunks for better context preservation\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = words[start:end]\n",
    "        if not chunk:\n",
    "            break\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "###############################################\n",
    "# Step 4: Embedding Using MedCPT Article Encoder\n",
    "###############################################\n",
    "# Load Article Encoder\n",
    "article_tokenizer = AutoTokenizer.from_pretrained(ARTICLE_ENCODER_MODEL)\n",
    "article_model = AutoModel.from_pretrained(ARTICLE_ENCODER_MODEL).to(DEVICE)\n",
    "article_model.eval()\n",
    "\n",
    "def embed_documents(doc_chunks):\n",
    "    # doc_chunks is a list of strings\n",
    "    # We'll pass them in batches to the model\n",
    "    all_embeds = []\n",
    "    batch_size = 8  # adjust as needed\n",
    "    for i in range(0, len(doc_chunks), batch_size):\n",
    "        batch = doc_chunks[i:i+batch_size]\n",
    "        with torch.no_grad():\n",
    "            encoded = article_tokenizer(batch, truncation=True, padding=True, return_tensors='pt', max_length=MAX_ARTICLE_LENGTH)\n",
    "            for k in encoded:\n",
    "                encoded[k] = encoded[k].to(DEVICE)\n",
    "            outputs = article_model(**encoded).last_hidden_state[:, 0, :]\n",
    "            # outputs: [batch, 768] embeddings\n",
    "            all_embeds.append(outputs.cpu().numpy())\n",
    "    if all_embeds:\n",
    "        return np.vstack(all_embeds)\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "###############################################\n",
    "# Step 5: Building the Vector Store (Faiss)\n",
    "###############################################\n",
    "def build_faiss_index(embeddings):\n",
    "    # embeddings should be a np.array of shape [N, 768]\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner product index\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "###############################################\n",
    "# Step 6: Process PDFs and Create the Index\n",
    "###############################################\n",
    "# We will create a metadata structure mapping each embedding to which doc and which chunk it came from\n",
    "doc_metadata = []\n",
    "doc_text_chunks = []\n",
    "doc_id = 0\n",
    "\n",
    "for filename in os.listdir(PDF_FOLDER):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        chunks = chunk_text(text)\n",
    "        for chunk_id, chunk in enumerate(chunks):\n",
    "            doc_metadata.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"filename\": filename,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "        doc_id += 1\n",
    "        doc_text_chunks.extend(chunks)\n",
    "\n",
    "# Compute embeddings for all chunks\n",
    "all_embeddings = embed_documents(doc_text_chunks)\n",
    "print(\"Total embeddings shape:\", all_embeddings.shape)\n",
    "\n",
    "# Build the Faiss index\n",
    "index = build_faiss_index(all_embeddings)\n",
    "\n",
    "# Save index and metadata if needed\n",
    "faiss.write_index(index, INDEX_SAVE_PATH)\n",
    "with open(DOC_META_PATH, 'w') as f:\n",
    "    json.dump(doc_metadata, f)\n",
    "\n",
    "###############################################\n",
    "# Step 7: Query the Index Using MedCPT Query Encoder\n",
    "###############################################\n",
    "query_tokenizer = AutoTokenizer.from_pretrained(QUERY_ENCODER_MODEL)\n",
    "query_model = AutoModel.from_pretrained(QUERY_ENCODER_MODEL).to(DEVICE)\n",
    "query_model.eval()\n",
    "\n",
    "def embed_query(queries):\n",
    "    with torch.no_grad():\n",
    "        encoded = query_tokenizer(queries, truncation=True, padding=True, return_tensors='pt', max_length=MAX_QUERY_LENGTH)\n",
    "        for k in encoded:\n",
    "            encoded[k] = encoded[k].to(DEVICE)\n",
    "        outputs = query_model(**encoded).last_hidden_state[:, 0, :]\n",
    "        return outputs.cpu().numpy()\n",
    "\n",
    "###############################################\n",
    "# Step 8: Re-rank the Results Using Cross Encoder\n",
    "###############################################\n",
    "# Load Cross Encoder\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL)\n",
    "cross_model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL).to(DEVICE)\n",
    "cross_model.eval()\n",
    "\n",
    "def rerank(query, candidates):\n",
    "    # Combine query and candidate text into pairs\n",
    "    pairs = [[query, candidate[\"text\"]] for candidate in candidates]\n",
    "    with torch.no_grad():\n",
    "        encoded = cross_tokenizer(pairs, truncation=True, padding=True, return_tensors=\"pt\", max_length=512)\n",
    "        for k in encoded:\n",
    "            encoded[k] = encoded[k].to(DEVICE)\n",
    "        logits = cross_model(**encoded).logits.squeeze(dim=1)  # Relevance scores\n",
    "    return logits.cpu().numpy()\n",
    "\n",
    "###############################################\n",
    "# Step 9: Combined Search with Re-ranking\n",
    "###############################################\n",
    "def search_with_rerank(query, k=5):\n",
    "    # Step 1: Dense retrieval using Faiss\n",
    "    query_embedding = embed_query([query])  # shape [1, 768]\n",
    "    scores, inds = index.search(query_embedding, k)\n",
    "\n",
    "    # Retrieve top-k candidates\n",
    "    candidates = []\n",
    "    for score, ind in zip(scores[0], inds[0]):\n",
    "        entry = doc_metadata[ind]\n",
    "        entry[\"retrieval_score\"] = float(score)\n",
    "        candidates.append(entry)\n",
    "\n",
    "    # Step 2: Re-rank using Cross Encoder\n",
    "    rerank_scores = rerank(query, candidates)\n",
    "    for i, score in enumerate(rerank_scores):\n",
    "        candidates[i][\"rerank_score\"] = float(score)\n",
    "\n",
    "    # Sort candidates by re-rank score\n",
    "    candidates = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "    return candidates\n",
    "\n",
    "###############################################\n",
    "# Step 10A: LLM-based Answer Generation\n",
    "###############################################\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "def get_llm_answer(query, retrieved_candidates):\n",
    "    \"\"\"\n",
    "    Use OpenAI GPT-4 to generate a concise answer from the retrieved text chunks.\n",
    "    \"\"\"\n",
    "    # Combine the top chunks into one context\n",
    "    context_text = \" \".join([cand[\"text\"] for cand in retrieved_candidates])\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a knowledgeable assistant. Use the context below to answer the question in one word or few.\n",
    "\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer (in one word or few):\n",
    "    \"\"\"\n",
    "    # Call OpenAI LLM (via llama_index)\n",
    "    llm = OpenAI(model=\"gpt-4\", temperature=0)  # or gpt-3.5-turbo\n",
    "    response = llm.complete(prompt)\n",
    "    return response , context_text\n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Example Usage\n",
    "###############################################\n",
    "# if __name__ == \"__main__\":\n",
    "#     sample_query = \"What metabolites are associated with diabetes and cancer?\"\n",
    "#     results = search_with_rerank(sample_query, k=5)\n",
    "\n",
    "#     print(\"Top results after re-ranking:\")\n",
    "#     for res in results:\n",
    "#         print(f\"Filename: {res['filename']}, Chunk: {res['chunk_id']}, \"\n",
    "#               f\"Retrieval Score: {res['retrieval_score']:.4f}, \"\n",
    "#               f\"Re-rank Score: {res['rerank_score']:.4f}\")\n",
    "#         print(\"Text snippet:\", res['text'][:200], \"...\")\n",
    "#         print(\"------\")\n",
    "\n",
    "#     # NEW: Get the final LLM-generated answer with minimal words\n",
    "#     final_response = get_llm_answer(sample_query, results)\n",
    "#     print(\"\\nFinal LLM Answer (in minimal words):\")\n",
    "#     print(final_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "###############################################\n",
    "# Step 10: Evaluation with ROUGE and BLEU (Updated)\n",
    "###############################################\n",
    "# Sample ground truth data (Replace this with your actual ground truth dataset)\n",
    "queries = [\n",
    "    \"Which biomarker is significantly elevated in the plasma of Gaucher Disease Type 1 patients?\",\n",
    "    \"What therapy reduces urinary GlcSph levels in Gaucher Disease patients?\",\n",
    "    \"Which biomarker is considered the gold standard for monitoring Gaucher Disease Type 1?\",\n",
    "    \"What technique is used to quantify lyso-Gb1 and its analogs in plasma?\",\n",
    "    \"Which urinary biomarker is highlighted for monitoring Gaucher Disease progression?\",\n",
    "    \"What urinary biomarker class is elevated in untreated Gaucher Disease patients?\",\n",
    "    \"What type of molecule is lyso-Gb1, which is associated with Gaucher Disease Type 1?\",\n",
    "    \"Which class of biomarkers does lyso-Gb1 belong to in the context of Gaucher Disease Type 1?\",\n",
    "    \"Which amino acids are elevated in patients with NASH compared to NAFLD?\",\n",
    "    \"What metabolic pathway is closely associated with NASH progression?\",\n",
    "    \"What urinary biomarker distinguishes between NAFLD and NASH?\",\n",
    "    \"What metabolic pathway is altered in the progression from NAFLD to NASH?\",\n",
    "    \"Which sulfated steroid increases with the progression of fibrosis in NAFLD?\",\n",
    "    \"Which metabolite ratio is associated with fibrosis severity in NAFLD?\",\n",
    "    \"Which biomarker is commonly used for early detection of Type 2 Diabetes (T2D)?\",\n",
    "    \"What metabolic pathway is associated with 3-hydroxybutyrate in T2D?\",\n",
    "    \"Which amino acids are identified as predictors of future diabetes in metabolomic studies?\",\n",
    "    \"What diagnostic method is used for metabolic profiling in diabetes research?\",\n",
    "    \"Which metabolite is associated with the progression of diabetic kidney disease?\",\n",
    "    \"What technology is used for identifying lipid metabolism-related biomarkers in diabetes?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Glucosylsphingosine (GlcSph).\",\n",
    "    \"Enzyme Replacement Therapy (ERT).\",\n",
    "    \"Lyso-Gb1.\",\n",
    "    \"UPLC-MS/MS.\",\n",
    "    \"Lyso-Gb1 analogs.\",\n",
    "    \"Polycyclic Lyso-Gb1 analogs.\",\n",
    "    \"A glucosylsphingosine derivative.\",\n",
    "    \"Lipid biomarkers.\",\n",
    "    \"Glutamate and phenylalanine.\",\n",
    "    \"Amino acid metabolism.\",\n",
    "    \"Pyroglutamic acid.\",\n",
    "    \"Pentose phosphate pathway.\",\n",
    "    \"16-OH-DHEA-S.\",\n",
    "    \"16-OH-DHEA-S/DHEA-S.\",\n",
    "    \"HbA1c.\",\n",
    "    \"Ketogenesis.\",\n",
    "    \"Branched-chain amino acids (BCAAs) like isoleucine, leucine, and valine.\",\n",
    "    \"Nuclear Magnetic Resonance (NMR) spectroscopy.\",\n",
    "    \"Phenylalanine.\",\n",
    "    \"Liquid Chromatography-Mass Spectrometry (LC-MS).\"\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_results_with_rerank(queries, ground_truths, k=3):\n",
    "    assert len(queries) == len(ground_truths), \"Queries and ground truths must have the same length.\"\n",
    "\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    smoothing_function = SmoothingFunction().method4  # For BLEU smoothing\n",
    "\n",
    "    results = []\n",
    "    for query, ground_truth in zip(queries, ground_truths):\n",
    "        # 1) Retrieve top results\n",
    "        retrieved_results = search_with_rerank(query, k=k)\n",
    "\n",
    "        # 2) Generate final LLM answer using the retrieved chunks\n",
    "        llm_answer , context_text = get_llm_answer(query, retrieved_results)\n",
    "        llm_answer = str(llm_answer)\n",
    "        response_text = str(llm_answer).strip()\n",
    "        # print(type(str(llm_answer)))\n",
    "\n",
    "        # 3) Evaluate the LLM's answer vs. ground truth\n",
    "        rouge_scores = rouge_scorer_instance.score(ground_truth, str(llm_answer))\n",
    "        bleu_score = sentence_bleu(\n",
    "            [ground_truth.split()],  # single reference\n",
    "            llm_answer.split(),      # LLM answer\n",
    "            smoothing_function=smoothing_function\n",
    "        )\n",
    "\n",
    "        # 4) Store metrics\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"response\": response_text[:],\n",
    "            \"retrieved_context\": context_text,\n",
    "            \"rouge1\": rouge_scores['rouge1'].fmeasure,\n",
    "            \"rouge2\": rouge_scores['rouge2'].fmeasure,\n",
    "            \"rougeL\": rouge_scores['rougeL'].fmeasure,\n",
    "            \"bleu\": bleu_score\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Evaluate the queries with the re-ranker\n",
    "evaluation_results = evaluate_results_with_rerank(queries, ground_truths)\n",
    "\n",
    "# Print results\n",
    "# for result in evaluation_results:\n",
    "#     print(\"Query:\", result[\"query\"])\n",
    "#     print(\"Ground Truth:\", result[\"ground_truth\"])\n",
    "#     print(\"Retrieved Text:\", result[\"retrieved_text\"][:200], \"...\")  # Limiting output for readability\n",
    "#     print(f\"ROUGE-1: {result['rouge1']:.4f}, ROUGE-2: {result['rouge2']:.4f}, ROUGE-L: {result['rougeL']:.4f}, BLEU: {result['bleu']:.4f}\")\n",
    "#     print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define the CSV file path\n",
    "output_csv_path = \"quantitative_evaluation_metrics_MedCPT_openAI.csv\"\n",
    "\n",
    "# Save evaluation results to a CSV file\n",
    "with open(output_csv_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile, escapechar='\\\\', quoting=csv.QUOTE_MINIMAL)  # Add escapechar\n",
    "\n",
    "    # Write header\n",
    "    writer.writerow([\"Query\", \"Ground Truth\", \"Response\", \"Retrieved Context\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\"])\n",
    "\n",
    "    # Write each evaluation result row\n",
    "    for result in evaluation_results:\n",
    "        writer.writerow([\n",
    "            result[\"query\"],\n",
    "            result[\"ground_truth\"],\n",
    "            result[\"response\"],\n",
    "            result[\"retrieved_context\"],  # Assuming this is a string\n",
    "            f\"{result['rouge1']:.4f}\",\n",
    "            f\"{result['rouge2']:.4f}\",\n",
    "            f\"{result['rougeL']:.4f}\",\n",
    "            f\"{result['bleu']:.4f}\"\n",
    "        ])\n",
    "\n",
    "print(f\"Evaluation metrics saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ragas\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness, context_recall, context_precision, answer_relevancy\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Convert `evaluation_results` to `ragas` format\n",
    "data_samples = {\n",
    "    'question': [result[\"query\"] for result in evaluation_results],\n",
    "    'answer': [result[\"response\"] for result in evaluation_results],  # Generated LLM responses\n",
    "    'contexts': [[result[\"retrieved_context\"]] for result in evaluation_results],  # List of retrieved contexts\n",
    "    'ground_truth': [result[\"ground_truth\"] for result in evaluation_results]  # Ground truth answers\n",
    "}\n",
    "\n",
    "# Step 2: Convert to `datasets.Dataset` object\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# Step 3: Evaluate using RAGAS metrics\n",
    "score = evaluate(dataset, metrics=[faithfulness, answer_correctness, context_recall, context_precision,answer_relevancy])\n",
    "\n",
    "# Step 4: Convert the scores to a Pandas DataFrame and save as CSV\n",
    "df = score.to_pandas()\n",
    "output_score_csv_path = \"qualitative_ragas_evaluation_scores_MedCPT_openAI.csv\"\n",
    "df.to_csv(output_score_csv_path, index=False)\n",
    "\n",
    "print(f\"Evaluation scores saved to {output_score_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
