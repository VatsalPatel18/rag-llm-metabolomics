{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-vector-stores-faiss\n",
    "!pip install llama_index llama-index-llms-openai\n",
    "!pip install PyPDF2\n",
    "!pip install faiss-cpu\n",
    "!pip install -U langchain-community\n",
    "!pip install rouge-score\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API Key\n",
    "openai_api_key =  # Replace with your actual key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "def test_llama_index_openai_llm(prompt, model=LLM_MODEL, temperature=TEMPERATURE):\n",
    "    \"\"\"\n",
    "    Query OpenAI LLM for direct question answering with concise responses.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Input prompt to query.\n",
    "        model (str): OpenAI model to use.\n",
    "        temperature (float): Temperature parameter for response generation.\n",
    "\n",
    "    Returns:\n",
    "        str: Model's response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Modify the prompt to ensure concise, one-word responses\n",
    "        concise_prompt = f\"{prompt}\\nAnswer in one word or less:\"\n",
    "\n",
    "        llm = OpenAI(model=model, temperature=temperature)\n",
    "        response = llm.complete(concise_prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example prompt\n",
    "    example_prompt = \"What is the capital of  France?\"\n",
    "\n",
    "    # Test the LLM\n",
    "    print(\"Testing LlamaIndex OpenAI LLM...\")\n",
    "    response = test_llama_index_openai_llm(example_prompt)\n",
    "    print(\"Response:\")\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: from \"/content/drive/MyDrive/sample_pdf_rag\" copy the folder and contents to current directory\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "source_path = \"/content/drive/MyDrive/sample_pdf_rag\"\n",
    "destination_path = os.getcwd()\n",
    "\n",
    "# Check if the source folder exists\n",
    "if os.path.exists(source_path):\n",
    "  try:\n",
    "    shutil.copytree(source_path, os.path.join(destination_path, \"sample_pdf_rag\"))\n",
    "    print(f\"Folder '{source_path}' copied to '{destination_path}' successfully.\")\n",
    "  except FileExistsError:\n",
    "      print(f\"Folder already exists in the destination directory\")\n",
    "  except OSError as e:\n",
    "    print(f\"Error copying folder: {e}\")\n",
    "else:\n",
    "  print(f\"Source folder '{source_path}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import json\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "###############################################\n",
    "# Step 1: Configuration\n",
    "###############################################\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = # place your Key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# General configurations\n",
    "PDF_FOLDER = \"./sample_pdf_rag\"  # Consistent folder naming\n",
    "PERSIST_DIR = \"./storage\"\n",
    "DOC_META_PATH = \"doc_metadata.json\"\n",
    "INDEX_SAVE_PATH = \"faiss_index.bin\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "LLM_MODEL = \"gpt-4\"\n",
    "\n",
    "# FAISS configuration\n",
    "dimension = 1536  # Embedding dimension for 'text-embedding-ada-002'\n",
    "CHUNK_SIZE = 500\n",
    "OVERLAP = 50\n",
    "\n",
    "###############################################\n",
    "# Step 2: Load Documents and Create Nodes\n",
    "###############################################\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract all text from a PDF file.\"\"\"\n",
    "    import PyPDF2\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Load documents\n",
    "if not os.path.exists(PDF_FOLDER):\n",
    "    raise FileNotFoundError(f\"Folder '{PDF_FOLDER}' not found.\")\n",
    "\n",
    "# Read and chunk documents\n",
    "documents = []\n",
    "doc_metadata = []\n",
    "doc_id = 0\n",
    "\n",
    "for filename in os.listdir(PDF_FOLDER):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "        # Chunking text\n",
    "        splitter = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP)\n",
    "        chunks = splitter.split_text(text)\n",
    "\n",
    "        for chunk_id, chunk in enumerate(chunks):\n",
    "            node = TextNode(text=chunk, id_=f\"{doc_id}_{chunk_id}\")\n",
    "            doc_metadata.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"filename\": filename,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "            documents.append(node)\n",
    "        doc_id += 1\n",
    "\n",
    "###############################################\n",
    "# Step 3: Setup FAISS Vector Store and Index\n",
    "###############################################\n",
    "# Initialize FAISS index\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "\n",
    "# Use OpenAI embeddings\n",
    "embedding_model = OpenAIEmbedding(model=EMBEDDING_MODEL)\n",
    "# vectorstore = FAISS.from_documents(doc_chunks, embeddings)\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR) or not os.listdir(PERSIST_DIR):\n",
    "    print(\"Building FAISS index...\")\n",
    "\n",
    "    # Create ingestion pipeline\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP),\n",
    "            embedding_model\n",
    "        ]\n",
    "    )\n",
    "    embeddings = [embedding_model.get_text_embedding(node.text) for node in documents]\n",
    "\n",
    "    for embed in embeddings:\n",
    "        faiss_index.add(torch.tensor(embed).unsqueeze(0).numpy())\n",
    "\n",
    "    # Save metadata and FAISS index\n",
    "    with open(DOC_META_PATH, 'w') as f:\n",
    "        json.dump(doc_metadata, f)\n",
    "    faiss.write_index(faiss_index, INDEX_SAVE_PATH)\n",
    "else:\n",
    "    print(\"Loading existing FAISS index...\")\n",
    "    faiss_index = faiss.read_index(INDEX_SAVE_PATH)\n",
    "\n",
    "###############################################\n",
    "# Step 4: Query Pipeline\n",
    "###############################################\n",
    "def retrieve_documents(query, top_k=2):\n",
    "    \"\"\"\n",
    "    Retrieve top_k documents for the given query using FAISS and embedding.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.get_text_embedding(query)\n",
    "    scores, indices = faiss_index.search(torch.tensor(query_embedding).unsqueeze(0).numpy(), top_k)\n",
    "\n",
    "    # Retrieve metadata for the top results\n",
    "    results = []\n",
    "    for i, score in zip(indices[0], scores[0]):\n",
    "        if i != -1:\n",
    "            results.append({\n",
    "                \"score\": score,\n",
    "                \"content\": doc_metadata[i][\"text\"],\n",
    "                \"metadata\": doc_metadata[i]\n",
    "            })\n",
    "    return results\n",
    "\n",
    "def query_with_llm(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Use LLM to answer the query based on retrieved documents.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(model=LLM_MODEL, temperature=0)\n",
    "    context = \"\\n\".join([doc[\"content\"] for doc in retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "    You are a knowledgeable assistant. Use the context below to answer the question concisely in as few words as possible.\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    Answer (in minimal words):\"\"\"\n",
    "    return llm.complete(prompt)\n",
    "\n",
    "###############################################\n",
    "# Example Usage\n",
    "###############################################\n",
    "# if __name__ == \"__main__\":\n",
    "#     user_query = \"What metabolites are associated with breast cancer?\"\n",
    "\n",
    "#     # Retrieve documents\n",
    "#     top_docs = retrieve_documents(user_query, top_k=5)\n",
    "#     print(\"Retrieved Documents:\")\n",
    "#     for doc in top_docs:\n",
    "#         print(doc[\"content\"][:200], \"...\")\n",
    "\n",
    "#     # Generate answer with LLM\n",
    "#     final_answer = query_with_llm(user_query, top_docs)\n",
    "#     print(\"\\nFinal Answer:\")\n",
    "#     print(final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "###############################################\n",
    "# Step 10: Evaluation with ROUGE and BLEU\n",
    "###############################################\n",
    "# Sample ground truth data (Replace this with your actual ground truth dataset)\n",
    "queries = [\n",
    "    \"Which biomarker is significantly elevated in the plasma of Gaucher Disease Type 1 patients?\",\n",
    "    \"What therapy reduces urinary GlcSph levels in Gaucher Disease patients?\",\n",
    "    \"Which biomarker is considered the gold standard for monitoring Gaucher Disease Type 1?\",\n",
    "    \"What technique is used to quantify lyso-Gb1 and its analogs in plasma?\",\n",
    "    \"Which urinary biomarker is highlighted for monitoring Gaucher Disease progression?\",\n",
    "    \"What urinary biomarker class is elevated in untreated Gaucher Disease patients?\",\n",
    "    \"What type of molecule is lyso-Gb1, which is associated with Gaucher Disease Type 1?\",\n",
    "    \"Which class of biomarkers does lyso-Gb1 belong to in the context of Gaucher Disease Type 1?\",\n",
    "    \"Which amino acids are elevated in patients with NASH compared to NAFLD?\",\n",
    "    \"What metabolic pathway is closely associated with NASH progression?\",\n",
    "    \"What urinary biomarker distinguishes between NAFLD and NASH?\",\n",
    "    \"What metabolic pathway is altered in the progression from NAFLD to NASH?\",\n",
    "    \"Which sulfated steroid increases with the progression of fibrosis in NAFLD?\",\n",
    "    \"Which metabolite ratio is associated with fibrosis severity in NAFLD?\",\n",
    "    \"Which biomarker is commonly used for early detection of Type 2 Diabetes (T2D)?\",\n",
    "    \"What metabolic pathway is associated with 3-hydroxybutyrate in T2D?\",\n",
    "    \"Which amino acids are identified as predictors of future diabetes in metabolomic studies?\",\n",
    "    \"What diagnostic method is used for metabolic profiling in diabetes research?\",\n",
    "    \"Which metabolite is associated with the progression of diabetic kidney disease?\",\n",
    "    \"What technology is used for identifying lipid metabolism-related biomarkers in diabetes?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Glucosylsphingosine (GlcSph).\",\n",
    "    \"Enzyme Replacement Therapy (ERT).\",\n",
    "    \"Lyso-Gb1.\",\n",
    "    \"UPLC-MS/MS.\",\n",
    "    \"Lyso-Gb1 analogs.\",\n",
    "    \"Polycyclic Lyso-Gb1 analogs.\",\n",
    "    \"A glucosylsphingosine derivative.\",\n",
    "    \"Lipid biomarkers.\",\n",
    "    \"Glutamate and phenylalanine.\",\n",
    "    \"Amino acid metabolism.\",\n",
    "    \"Pyroglutamic acid.\",\n",
    "    \"Pentose phosphate pathway.\",\n",
    "    \"16-OH-DHEA-S.\",\n",
    "    \"16-OH-DHEA-S/DHEA-S.\",\n",
    "    \"HbA1c.\",\n",
    "    \"Ketogenesis.\",\n",
    "    \"Branched-chain amino acids (BCAAs) like isoleucine, leucine, and valine.\",\n",
    "    \"Nuclear Magnetic Resonance (NMR) spectroscopy.\",\n",
    "    \"Phenylalanine.\",\n",
    "    \"Liquid Chromatography-Mass Spectrometry (LC-MS).\"\n",
    "]\n",
    "\n",
    "\n",
    "# def evaluate_results_with_llm(queries, ground_truths):\n",
    "#     \"\"\"\n",
    "#     Evaluate LLM-generated responses with ROUGE and BLEU metrics.\n",
    "\n",
    "#     Parameters:\n",
    "#     - queries: List of queries.\n",
    "#     - ground_truths: List of ground truth answers.\n",
    "\n",
    "#     Returns:\n",
    "#     - results: List of evaluation metrics for each query.\n",
    "#     \"\"\"\n",
    "#     # If you want to be absolutely sure they match in length, you can re-enable or remove this check:\n",
    "#     # assert len(queries) == len(ground_truths), \"Queries and ground truths must have the same length.\"\n",
    "\n",
    "#     # Initialize scorers\n",
    "#     rouge_scorer_instance = rouge_scorer.RougeScorer(\n",
    "#         ['rouge1', 'rouge2', 'rougeL'],\n",
    "#         use_stemmer=True\n",
    "#     )\n",
    "#     smoothing_function = SmoothingFunction().method4  # For BLEU smoothing\n",
    "\n",
    "#     results = []\n",
    "#     # Safely zip up to the shortest list; or just zip if you're sure they're the same length\n",
    "#     for query, ground_truth in zip(queries, ground_truths):\n",
    "#         # 1) Retrieve documents from your RAG pipeline\n",
    "#         retrieved_docs = retrieve_documents(query, top_k=3)\n",
    "\n",
    "#         # 2) Generate an LLM response based on the retrieved docs\n",
    "#         response = query_with_llm(query, retrieved_docs)\n",
    "\n",
    "#         # 3) Convert the response to a plain string\n",
    "#         #    If response is already a string, no change needed. Otherwise convert.\n",
    "#         #    For example, if your LLM returns a dictionary or an object, adapt accordingly.\n",
    "#         response_text = str(response).strip()\n",
    "\n",
    "#         # 4) Calculate ROUGE\n",
    "#         rouge_scores = rouge_scorer_instance.score(ground_truth, response_text)\n",
    "\n",
    "#         # 5) Calculate BLEU\n",
    "#         bleu_score = sentence_bleu(\n",
    "#             [ground_truth.split()],   # Reference\n",
    "#             response_text.split(),    # Candidate\n",
    "#             smoothing_function=smoothing_function\n",
    "#         )\n",
    "\n",
    "#         # 6) Store the results\n",
    "#         results.append({\n",
    "#             \"query\": query,\n",
    "#             \"ground_truth\": ground_truth,\n",
    "#             \"response\": response_text,\n",
    "#             \"rouge1\": rouge_scores['rouge1'].fmeasure,\n",
    "#             \"rouge2\": rouge_scores['rouge2'].fmeasure,\n",
    "#             \"rougeL\": rouge_scores['rougeL'].fmeasure,\n",
    "#             \"bleu\": bleu_score\n",
    "#         })\n",
    "\n",
    "#     return results\n",
    "\n",
    "def evaluate_results_with_llm(queries, ground_truths, top_k=2):\n",
    "    \"\"\"\n",
    "    Evaluate LLM-generated responses using ROUGE, BLEU, and include retrieved context.\n",
    "    \"\"\"\n",
    "    # For ROUGE\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(\n",
    "        ['rouge1', 'rouge2', 'rougeL'],\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    # For BLEU smoothing\n",
    "    smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "    results = []\n",
    "    for query, ground_truth in zip(queries, ground_truths):\n",
    "        # 1) Retrieve relevant docs\n",
    "        retrieved_docs = retrieve_documents(query, top_k=top_k)\n",
    "\n",
    "        # 2) Generate context from retrieved documents\n",
    "        context_text = \"\\n\".join([doc[\"content\"] for doc in retrieved_docs])\n",
    "\n",
    "        # 3) Generate LLM response\n",
    "        response = query_with_llm(query, retrieved_docs)\n",
    "        response_text = str(response).strip()\n",
    "\n",
    "        # 4) Compute ROUGE\n",
    "        rouge_scores = rouge_scorer_instance.score(ground_truth, response_text)\n",
    "\n",
    "        # 5) Compute BLEU\n",
    "        bleu_score = sentence_bleu(\n",
    "            [ground_truth.split()],    # reference\n",
    "            response_text.split(),     # candidate\n",
    "            smoothing_function=smoothing_function,\n",
    "            weights=(0.5, 0.5, 0, 0)\n",
    "        )\n",
    "\n",
    "        # 6) Store the results\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"response\": response_text[:],  # Truncate for readability\n",
    "            \"retrieved_context\": context_text,  # <-- Add retrieved context\n",
    "            \"rouge1\": rouge_scores['rouge1'].fmeasure,\n",
    "            \"rouge2\": rouge_scores['rouge2'].fmeasure,\n",
    "            \"rougeL\": rouge_scores['rougeL'].fmeasure,\n",
    "            \"bleu\": bleu_score\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Run Evaluation\n",
    "###############################################\n",
    "evaluation_results = evaluate_results_with_llm(queries, ground_truths)\n",
    "\n",
    "import csv\n",
    "\n",
    "# # Define the CSV file path\n",
    "# output_csv_path = \"evaluation_metrics.csv\"\n",
    "\n",
    "# # Collect evaluation results into a CSV\n",
    "# with open(output_csv_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     # Write header\n",
    "#     writer.writerow([\"Query\", \"Ground Truth\", \"Response\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\"])\n",
    "\n",
    "#     # Write each evaluation result row\n",
    "#     for result in evaluation_results:\n",
    "#         writer.writerow([\n",
    "#             result[\"query\"],\n",
    "#             result[\"ground_truth\"],\n",
    "#             result[\"response\"],  # You may want to truncate this if responses are too long\n",
    "#             f\"{result['rouge1']:.4f}\",\n",
    "#             f\"{result['rouge2']:.4f}\",\n",
    "#             f\"{result['rougeL']:.4f}\",\n",
    "#             f\"{result['bleu']:.4f}\"\n",
    "#         ])\n",
    "\n",
    "# print(f\"Evaluation metrics saved to {output_csv_path}\")\n",
    "\n",
    "# Define the CSV file path\n",
    "output_csv_path = \"evaluation_metrics_with_context.csv\"\n",
    "\n",
    "# Collect evaluation results into a CSV\n",
    "with open(output_csv_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write header\n",
    "    writer.writerow([\"Query\", \"Ground Truth\", \"Response\", \"Retrieved Context\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\"])\n",
    "\n",
    "    # Write each evaluation result row\n",
    "    for result in evaluation_results:\n",
    "        writer.writerow([\n",
    "            result[\"query\"],\n",
    "            result[\"ground_truth\"],\n",
    "            result[\"response\"],  # You may want to truncate this if responses are too long\n",
    "            result[\"retrieved_context\"],  # <-- Add retrieved context\n",
    "            f\"{result['rouge1']:.4f}\",\n",
    "            f\"{result['rouge2']:.4f}\",\n",
    "            f\"{result['rougeL']:.4f}\",\n",
    "            f\"{result['bleu']:.4f}\"\n",
    "        ])\n",
    "\n",
    "print(f\"Evaluation metrics saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "# # Print evaluation results\n",
    "# for result in evaluation_results:\n",
    "#     print(\"Query:\", result[\"query\"])\n",
    "#     print(\"Ground Truth:\", result[\"ground_truth\"])\n",
    "#     # Truncate response for readability\n",
    "#     print(\"Response:\", result[\"response\"][:200], \"...\")\n",
    "#     print(f\"ROUGE-1: {result['rouge1']:.4f}, \"\n",
    "#           f\"ROUGE-2: {result['rouge2']:.4f}, \"\n",
    "#           f\"ROUGE-L: {result['rougeL']:.4f}, \"\n",
    "#           f\"BLEU: {result['bleu']:.4f}\")\n",
    "#     print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "output_csv_path = \"quantitative_evaluation_metrics_with_context_4openai.csv\"\n",
    "\n",
    "# Collect evaluation results into a CSV\n",
    "with open(output_csv_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL, escapechar='\\\\')\n",
    "    # Write header\n",
    "    writer.writerow([\"Query\", \"Ground Truth\", \"Response\", \"Retrieved Context\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\"])\n",
    "\n",
    "    # Write each evaluation result row\n",
    "    for result in evaluation_results:\n",
    "        writer.writerow([\n",
    "            result[\"query\"],\n",
    "            result[\"ground_truth\"],\n",
    "            result[\"response\"],  # You may want to truncate this if responses are too long\n",
    "            result.get(\"retrieved_context\", \"\"),  # Add retrieved context if available; use empty string if not\n",
    "            f\"{result['rouge1']:.4f}\",\n",
    "            f\"{result['rouge2']:.4f}\",\n",
    "            f\"{result['rougeL']:.4f}\",\n",
    "            f\"{result['bleu']:.4f}\"\n",
    "        ])\n",
    "\n",
    "print(f\"Evaluation metrics saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness, context_recall, context_precision, answer_relevancy\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Convert `evaluation_results` to `ragas` format\n",
    "data_samples = {\n",
    "    'question': [result[\"query\"] for result in evaluation_results],\n",
    "    'answer': [result[\"response\"] for result in evaluation_results],  # Generated LLM responses\n",
    "    'contexts': [[result[\"retrieved_context\"]] for result in evaluation_results],  # List of retrieved contexts\n",
    "    'ground_truth': [result[\"ground_truth\"] for result in evaluation_results]  # Ground truth answers\n",
    "}\n",
    "\n",
    "# Step 2: Convert to `datasets.Dataset` object\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# Step 3: Evaluate using RAGAS metrics\n",
    "score = evaluate(dataset, metrics=[faithfulness, answer_correctness, context_recall, context_precision,answer_relevancy])\n",
    "\n",
    "# Step 4: Convert the scores to a Pandas DataFrame and save as CSV\n",
    "df = score.to_pandas()\n",
    "output_score_csv_path = \"qualitative_ragas_evaluation_scores_4openai.csv\"\n",
    "df.to_csv(output_score_csv_path, index=False)\n",
    "\n",
    "print(f\"Evaluation scores saved to {output_score_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import LLMContextPrecisionWithReference\n",
    "\n",
    "context_precision = LLMContextPrecisionWithReference()\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    reference=\"The Eiffel Tower is located in Paris.\",\n",
    "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"],\n",
    ")\n",
    "\n",
    "await context_precision.single_turn_ascore(sample)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
